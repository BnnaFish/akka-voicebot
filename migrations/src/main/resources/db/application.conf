akka.persistence {
  journal.plugin = "postgres-journal"
  snapshot-store.plugin = "postgres-snapshot-store"
}
postgres-journal.dao = "akka.persistence.postgres.journal.dao.PartitionsJournalDao"

mydb = {
  connectionPool = "HikariCP" //use HikariCP for our connection pool
  dataSourceClass = "org.postgresql.ds.PGSimpleDataSource" //Simple datasource with no connection pooling. The connection pool has already been specified with HikariCP.
  properties = {
    serverName = "localhost"
    portNumber = "5432"
    databaseName = "voicebot"
    user = "docker"
    password = "docker"
  }
  numThreads = 10
}
# the akka-persistence-journal in use
postgres-journal {
  class = "akka.persistence.postgres.journal.PostgresAsyncWriteJournal"

  tables {
    journal {
      # Used by akka.persistence.postgres.journal.dao.PartitionsJournalDao
      # and akka.persistence.postgres.journal.dao.PartitionedJournalDao
      partitions {
        # The size of a single partition
        size: 10000000
        # Partition name prefix
        prefix: "j"
      }
      tableName = "journal"
      schemaName = ""
      columnNames {
        ordering = "ordering"
        deleted = "deleted"
        persistenceId = "persistence_id"
        sequenceNumber = "sequence_number"
        created = "created"
        tags = "tags"
        message = "message"
        metadata = "metadata"
      }
    }

    tags {
      tableName = "tags"
      schameName = ""
      columnNames {
        id = "id"
        name = "name"
      }
    }
  }

  dao = "akka.persistence.postgres.journal.dao.PartitionsJournalDao"

  # Configuration for akka.persistence.postgres.tag.TagIdResolver
  tags {
    # The timeout after unused tag is removed from the TagIdResolver's cache
    cacheTtl = 1 hour
    # Used in case of multiple sessions trying to add the same tag-label-to-id mapping at the same time.
    insertionRetryAttempts = 1
  }

  # The size of the buffer used when queueing up events for batch writing. This number must be bigger then the number
  # of events that may be written concurrently. In other words this number must be bigger than the number of persistent
  # actors that are actively persisting at the same time.
  bufferSize = 1000
  # The maximum size of the batches in which journal rows will be inserted
  batchSize = 400
  # The maximum size of the batches in which journal rows will be read when recovering
  replayBatchSize = 400
  # The maximum number of batch-inserts that may be running concurrently
  parallelism = 8

  # This setting can be used to configure usage of a shared database.
  # To disable usage of a shared database, set to null or an empty string.
  # When set to a non empty string, this setting does two things:
  # - The actor which manages the write-journal will not automatically close the db when the actor stops (since it is shared)
  # - If akka-persistence-postgres.database-provider-fqcn is set to akka.persistence.postgres.db.DefaultSlickDatabaseProvider
  #   then the shared database with the given name will be used. (shared databases are configured as part of akka-persistence-postgres.shared-databases)
  #   Please note that the database will only be shared with the other journals if the use-shared-db is also set
  #   to the same value for these other journals.
  use-shared-db = null

  slick {

    db {
      connectionPool = "HikariCP"

      url = "jdbc:postgresql://localhost:5432/voicebot"
      user = "docker"
      password = "docker"

      # hikariCP settings; see: https://github.com/brettwooldridge/HikariCP
      # Slick will use an async executor with a fixed size queue of 10.000 objects
      # The async executor is a connection pool for asynchronous execution of blocking I/O actions.
      # This is used for the asynchronous query execution API on top of blocking back-ends like JDBC.
      queueSize = 10000 // number of objects that can be queued by the async executor

      # This property controls the maximum number of milliseconds that a client (that's you) will wait for a connection
      # from the pool. If this time is exceeded without a connection becoming available, a SQLException will be thrown.
      # 1000ms is the minimum value. Default: 180000 (3 minutes)
      connectionTimeout = 180000

      # This property controls the maximum amount of time that a connection will be tested for aliveness.
      # This value must be less than the connectionTimeout. The lowest accepted validation timeout is 1000ms (1 second). Default: 5000
      validationTimeout = 5000

      # 10 minutes: This property controls the maximum amount of time that a connection is allowed to sit idle in the pool.
      # Whether a connection is retired as idle or not is subject to a maximum variation of +30 seconds, and average variation
      # of +15 seconds. A connection will never be retired as idle before this timeout. A value of 0 means that idle connections
      # are never removed from the pool. Default: 600000 (10 minutes)
      idleTimeout = 600000

      # 30 minutes: This property controls the maximum lifetime of a connection in the pool. When a connection reaches this timeout
      # it will be retired from the pool, subject to a maximum variation of +30 seconds. An in-use connection will never be retired,
      # only when it is closed will it then be removed. We strongly recommend setting this value, and it should be at least 30 seconds
      # less than any database-level connection timeout. A value of 0 indicates no maximum lifetime (infinite lifetime),
      # subject of course to the idleTimeout setting. Default: 1800000 (30 minutes)
      maxLifetime = 1800000

      # This property controls the amount of time that a connection can be out of the pool before a message is logged indicating a
      # possible connection leak. A value of 0 means leak detection is disabled.
      # Lowest acceptable value for enabling leak detection is 2000 (2 secs). Default: 0
      leakDetectionThreshold = 0

      # ensures that the database does not get dropped while we are using it
      keepAliveConnection = on

      # See some tips on thread/connection pool sizing on https://github.com/brettwooldridge/HikariCP/wiki/About-Pool-Sizing
      # Keep in mind that the number of threads must equal the maximum number of connections.
      numThreads = 1
      maxConnections = 1 // 2 * numThreads + 1
      minConnections = 1
    }
  }
}

# the akka-persistence-query provider in use
postgres-read-journal {
  class = "akka.persistence.postgres.query.PostgresReadJournalProvider"

  # Absolute path to the write journal plugin configuration section.
  # Read journal uses event adapters from the write plugin
  # to adapt events.
  write-plugin = "postgres-journal"

  # New events are retrieved (polled) with this interval.
  refresh-interval = "1s"

  # How many events to fetch in one query (replay) and keep buffered until they
  # are delivered downstreams.
  max-buffer-size = "500"

  # If enabled, automatically close the database connection when the actor system is terminated
  add-shutdown-hook = true

  # This setting can be used to configure usage of a shared database.
  # To disable usage of a shared database, set to null or an empty string.
  # This setting only has effect if akka-persistence-postgres.database-provider-fqcn is set to
  # akka.persistence.postgres.db.DefaultSlickDatabaseProvider. When this setting is set to a non empty string
  # then the shared database with the given name will be used. (shared databases are configured as part of akka-persistence-postgres.shared-databases)
  # Please note that the database will only be shared with the other journals if the use-shared-db is also set
  # to the same value for these other journals.
  use-shared-db = null

  dao = "akka.persistence.postgres.query.dao.ByteArrayReadJournalDao"

  # Configuration for akka.persistence.postgres.tag.TagIdResolver
  tags {
    # The timeout after unused tag is removed from the TagIdResolver's cache
    cacheTtl = 1 hour
  }

  # if true, queries will include logically deleted events
  # should not be configured directly, but through property akka-persistence-postgres.logicalDelete.enable
  # in order to keep consistent behavior over write/read sides
  includeLogicallyDeleted = ${akka-persistence-postgres.logicalDeletion.enable}

  # Settings for determining if ids (ordering column) in the journal are out of sequence.
  journal-sequence-retrieval {
    # The maximum number of ids that will be retrieved in each batch
    batch-size = 10000
    # In case a number in the sequence is missing, this is the amount of retries that will be done to see
    # if the number is still found. Note that the time after which a number in the sequence is assumed missing is
    # equal to maxTries * queryDelay
    # (maxTries may not be zero)
    max-tries = 10
    # How often the actor will query for new data
    query-delay = 1 second
    # The maximum backoff time before trying to query again in case of database failures
    max-backoff-query-delay = 1 minute
    # The ask timeout to use when querying the journal sequence actor, the actor should normally repond very quickly,
    # since it always replies with its current internal state
    ask-timeout = 1 second
  }

  tables {
    journal {
      tableName = "journal"
      schemaName = ""
      columnNames {
        ordering = "ordering"
        persistenceId = "persistence_id"
        sequenceNumber = "sequence_number"
        created = "created"
        tags = "tags"
        message = "message"
      }
    }

    tags {
      tableName = "tags"
      schameName = ""
      columnNames {
        id = "id"
        name = "name"
      }
    }
  }

  slick = ${postgres-journal.logicalDeletion.enable}
}
